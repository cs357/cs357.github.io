<!doctype html>
<html lang="en">
<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Eigenvalues and Eigenvectors - CS 357</title>
    <link rel="icon" href="//assets/img/favicon.png">
    <link rel="shortcut icon" href="//assets/img/favicon.png" />
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="//assets/css/bootstrap.min.css">
    <!-- animate CSS -->
    <link rel="stylesheet" href="//assets/css/animate.css">
    <!-- owl carousel CSS -->
    <link rel="stylesheet" href="//assets/css/owl.carousel.min.css">
    <!-- themify CSS -->
    <link rel="stylesheet" href="//assets/css/themify-icons.css">
    <!-- flaticon CSS -->
    <link rel="stylesheet" href="//assets/css/flaticon.css">
    <!-- font awesome CSS -->
    <link rel="stylesheet" href="//assets/css/magnific-popup.css">
    <!-- swiper CSS -->
    <link rel="stylesheet" href="//assets/css/slick.css">
    <!-- style CSS -->
    <link rel="stylesheet" href="//assets/css/style.css">
    <!-- opengraph -->
    <meta property="og:site_name" content="CS 357 Fall 2024" />
    <meta property="og:title" content="Eigenvalues and Eigenvectors - CS 357" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="//assets/img/photos/group-CIF.png" />
    <!-- Mathjax Support -->
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
    </script>

    
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-157880752-1', 'auto');
        ga('send', 'pageview');
    </script>
    
</head>

<nav class="main_menu home_menu">
    <div class="container">
        <div class="row align-items-center">
            <div class="col-lg-12">
                <nav class="navbar navbar-expand-lg navbar-light">
                    <a class="navbar-brand" href="/">
                        
                            <img class="mr-2" style="max-height:25pt" src="//assets/img/logo.png" alt="logo"> CS 357 <span class="badge badge-success">Fall 2024</span>
                        
                    </a>
                    <button class="navbar-toggler" type="button" data-toggle="collapse"
                            data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                            aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>

                    <div class="collapse navbar-collapse main-menu-item justify-content-end"
                         id="navbarSupportedContent">
                        <ul class="navbar-nav align-items-center">
                            <li class="nav-item ignore-md-custom-style">
                                <a class="nav-link" href="/pages/syllabus.html">Syllabus</a>
                            </li>
                            <li class="nav-item ignore-md-custom-style">
                                <a class="nav-link" href="/pages/schedule.html">Schedule</a>
                            </li>
                            <li class="nav-item ignore-md-custom-style">
                                <a class="nav-link" href="/pages/contact.html">Support</a>
                            </li>
                            <li class="nav-item ignore-md-custom-style">
                                <a class="nav-link" href="/pages/lectures.html">Group Activities</a>
                            </li>
                            <li class="nav-item ignore-md-custom-style">
                                <a class="nav-link" href="/pages/quizzes.html">Quizzes</a>
                            </li>
                             <!--Mobile view has different textbook button. -->
                            <li class="d-lg-none nav-item ignore-md-custom-style">
                                <a class="nav-link" href="https://cs357.github.io/textbook/">Online Textbook</a>
                            </li>
                            <li class="d-none d-lg-block ignore-md-custom-style">
                                <a class="btn_1" href="https://cs357.github.io/textbook/">Online Textbook</a>
                            </li>
                        </ul>
                    </div>
                </nav>
            </div>
        </div>
        
    </div>
</nav>

<body>
<style>
    p:not(.ignore-md-custom-style) {
        line-height: unset;
        margin-bottom: 1em;
        color: black;
    }

    ul {
        all: unset;
    }

    li:not(.ignore-md-custom-style) {
        margin-left: 20pt;
    }

    h2:not(.ignore-md-custom-style) {
        margin-top: 2em;
        font-weight: 500;
    }

    h3:not(.ignore-md-custom-style) {
        margin-top: 1em;
    }
</style>
<section class="section_padding">
    <div class="container text-dark">
        <h1 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h1>

<hr />

<h2 id="learning-objectives">Learning Objectives</h2>

<ul>
  <li>Compute eigenvalue/eigenvector for various applications.</li>
  <li>Use the Power Method to find an eigenvector.</li>
  <li>Definition of eigenvalue/eigenvectors.</li>
  <li>Methods of obtaining eigenvalues.</li>
  <li>Computing eigenvalue/eigenvectors for various applications.</li>
  <li>Using the Power Method to find an eigenvector.</li>
</ul>

<h2 id="eigenvalues-and-eigenvectors-1">Eigenvalues and Eigenvectors</h2>

<p>An <strong><em>eigenvalue</em></strong> of an \(n \times n\) matrix \(\mathbf{A}\) is a scalar \(\lambda\) such that
\(\mathbf{A} {\bf x} = \lambda {\bf x}\)
for some non-zero vector \({\bf x}\). The eigenvalue \(\lambda\) can be any real or complex scalar, (which we write \(\lambda \in \mathbb{R}\ \text{or } \lambda \in \mathbb{C}\)). Eigenvalues can be complex even if all the entries of the matrix \(\mathbf{A}\) are real. In this case, the corresponding vector \({\bf x}\) must have complex-valued components (which we write \({\bf x}\in \mathbb{C}^n\)). The equation \(\mathbf{A}\mathbf{x}=\lambda\mathbf{x}\) is called the <strong><em>eigenvalue equation</em></strong> and any such non-zero vector \({\bf x}\) is called an <strong><em>eigenvector</em></strong> of \(\mathbf{A}\) corresponding to \(\lambda\).</p>

<p>The eigenvalue equation can be rearranged to \((\mathbf{A} - \lambda {\bf I}) {\bf x} = 0\), and because \({\bf x}\) is not zero this has solutions if and only if \(\lambda\) is a solution of the <strong><em>characteristic equation</em></strong>:</p>

\[\operatorname{det}(\mathbf{A} - \lambda {\bf I}) = 0.\]

<p>The expression \(p(\lambda) = \operatorname{det}(\mathbf{A} - \lambda {\bf I})\) is called the <strong><em>characteristic polynomial</em></strong> and is a polynomial of degree <span>\(n\)</span>.</p>

<p>Although all eigenvalues can be found by solving the characteristic equation, there is no general, closed-form analytical solution for the roots of polynomials of degree \(n \ge 5\) and this is not a good numerical approach for finding eigenvalues.</p>

<p>Unless otherwise specified, we write eigenvalues ordered by magnitude, so that</p>

\[|\lambda_1| \geq |\lambda_2| \geq \cdots \geq |\lambda_n|,\]

<p>and we normalize eigenvectors, so that \(\|{\bf x}\| = 1\).</p>

<h2 id="eigenvalues-of-a-shifted-matrix">Eigenvalues of a Shifted Matrix</h2>

<p>Given a matrix \(\mathbf{A}\), for any constant scalar \(\sigma\), we define the <strong><em>shifted matrix</em></strong> is \(\mathbf{A} - \sigma {\bf I}\). If \(\lambda\) is an eigenvalue of \(\mathbf{A}\) with eigenvector \({\bf x}\) then \(\lambda - \sigma\) is an eigenvalue of the shifted matrix with the same eigenvector. This can be derived by</p>

\[\begin{aligned} (\mathbf{A} - \sigma {\bf I}) {\bf x} &amp;= \mathbf{A} {\bf x} - \sigma {\bf I} {\bf x} \\ &amp;= \lambda {\bf x} - \sigma {\bf x} \\ &amp;= (\lambda - \sigma) {\bf x}. \end{aligned}\]

<h2 id="eigenvalues-of-an-inverse">Eigenvalues of an Inverse</h2>

<p>An invertible matrix cannot have an eigenvalue equal to zero. Furthermore, the eigenvalues of the inverse matrix are equal to the inverse of the eigenvalues of the original matrix:</p>

\[\mathbf{A} {\bf x} = \lambda {\bf x}\implies \\ \mathbf{A}^{-1} \mathbf{A} {\bf x} = \lambda \mathbf{A}^{-1} {\bf x} \implies \\ {\bf x} = \lambda \mathbf{A}^{-1} {\bf x}\implies \\ \mathbf{A}^{-1} {\bf x} = \frac{1}{\lambda} {\bf x}.\]

<h2 id="eigenvalues-of-a-shifted-inverse">Eigenvalues of a Shifted Inverse</h2>

<p>Similarly, we can describe the eigenvalues for shifted inverse matrices as:</p>

\[(\mathbf{A} - \sigma {\bf I})^{-1} {\bf x} = \frac{1}{\lambda - \sigma} {\bf x}.\]

<p>It is important to note here, that the eigenvectors remain unchanged for shifted or/and inverted matrices.</p>

<h2 id="diagonalizability">Diagonalizability</h2>

<p>An \(n \times n\) matrix with <span>\(n\)</span> linearly independent eigenvectors can be expressed as its eigenvalues and eigenvectors as:</p>

\[\mathbf{A}\mathbf{X} = \begin{bmatrix} \vert &amp; &amp; \vert \\ \lambda_1 {\bf x}_1 &amp; \dots &amp; \lambda_n {\bf x}_n \\ \vert &amp; &amp; \vert \end{bmatrix} = \begin{bmatrix} \vert &amp; &amp; \vert \\ {\bf x}_1 &amp; \dots &amp; {\bf x}_n \\ \vert &amp; &amp; \vert \end{bmatrix} \begin{bmatrix}\lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_n \end{bmatrix} = \mathbf{X}\mathbf{D}\]

<p>The eigenvector matrix can be inverted to obtain the following <strong><em>similarity transformation</em></strong> of \(\mathbf{A}\):</p>

\[\mathbf{AX} = \mathbf{XD} \iff \mathbf{A} = \mathbf{XDX}^{-1} \iff \mathbf{X}^{-1}\mathbf{A}\mathbf{X} = \mathbf{D}\]

<p>Multiplying the matrix \(\mathbf{A}\) by \(\mathbf{X}^{-1}\) on the left and \(\mathbf{X}\) on the right transforms it into a diagonal matrix; it has been ‘‘diagonalized’’.</p>

<h4 id="example-matrix-that-is-diagonalizable">Example: Matrix that is diagonalizable</h4>

<p>An \(n \times n\) matrix is diagonalizable if and only if it has <span>\(n\)</span> linearly independent eigenvectors. For example:</p>

\[\overbrace{\begin{bmatrix} 1/6 &amp; -1/3 &amp; 1/6 \\ -1/2 &amp; 0 &amp; 1/2 \\ 1/3 &amp; 1/3 &amp; 1/3 \end{bmatrix}}^{\mathbf{X}^{-1}} \overbrace{\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}}^{\mathbf{A}} \overbrace{\begin{bmatrix} 1 &amp; -1 &amp; 1 \\ -2 &amp; 0 &amp; 1 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}}^{\mathbf{X}} = \overbrace{\begin{bmatrix} -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}}^{\mathbf{D}}\]

<h4 id="example-matrix-that-is-not-diagonalizable">Example: Matrix that is not diagonalizable</h4>

<p>A matrix \(\mathbf{A}\) with linearly dependent eigenvectors is not diagonalizable. For example, while it is true that</p>

\[\overbrace{\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}}^{\mathbf{A}} \overbrace{\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}}^{\mathbf{X}} = \overbrace{\begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}}^{\mathbf{X}} \overbrace{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}}^{\mathbf{D}},\]

<p>the matrix \(\mathbf{X}\) does not have an inverse, so we cannot diagonalize \(\mathbf{A}\) by applying an inverse. In fact, for any non-singular matrix \(\mathbf{P}\), the product \(\mathbf{P}^{-1}\mathbf{AP}\) is not diagonal.</p>

<h4 id="things-to-remember-about-eigenvalues">Things to Remember About Eigenvalues</h4>

<ul>
  <li>Eigenvalues can have zero value.</li>
  <li>Eigenvalues can be negative.</li>
  <li>Eigenvalues can be real or complex numbers.</li>
  <li>An \(n \times n\) real matrix can have complex eigenvalues.</li>
  <li>The eigenvalues of an \(n \times n\) matrix are not necessarily unique. In fact, we can define the multiplicity of an eigenvalue.</li>
  <li>If an \(n \times n\) matrix has \(n\) linearly independent eigenvectors, then the matrix is diagonalizable.</li>
</ul>

<h2 id="expressing-an-arbitrary-vector-as-a-linear-combination-of-eigenvectors">Expressing an Arbitrary Vector as a Linear Combination of Eigenvectors</h2>

<p>If an \(n\times n\) matrix \(\mathbf{A}\) is diagonalizable, then we can write an arbitrary vector as a linear combination of the eigenvectors of \(\mathbf{A}\). Let \({\bf u}_1,{\bf u}_2,\dots,{\bf u}_n\) be <span>\(n\)</span> linearly independent eigenvectors of \(\mathbf{A}\); then an arbitrary vector \(\mathbf{x}_0\) can be written:</p>

\[{\bf x}_0 = \alpha_1 {\bf u}_1 + \alpha_2 {\bf u}_2 + \dots + \alpha_n {\bf u}_n.\]

<p>If we apply the matrix \(\mathbf{A}\) to \(\mathbf{x}_0\):</p>

\[\begin{align} \mathbf{A}{\bf x}_0 &amp;= \alpha_1 \mathbf{A}{\bf u}_1 + \alpha_2\mathbf{A}{\bf u}_2 + \dots + \alpha_n \mathbf{A}{\bf u}_n, \\ &amp;= \alpha_1 \lambda_1 {\bf u}_1 + \alpha_2\lambda_2 {\bf u}_2 + \dots + \alpha_n \lambda_n {\bf u}_n, \\ &amp;= \lambda_1 \left(\alpha_1 {\bf u}_1 + \alpha_2\frac{\lambda_2}{\lambda_1}{\bf u}_2 + \dots + \alpha_n \frac{\lambda_n}{\lambda_1} {\bf u}_n\right). \\ \end{align}\]

<p>If we repeatedly apply \(\mathbf{A}\) we have</p>

\[\mathbf{A}^k{\bf x}_0= \lambda_1^k \left(\alpha_1 {\bf u}_1 + \alpha_2\left(\frac{\lambda_2}{\lambda_1}\right)^k{\bf u}_2 + \dots + \alpha_n \left(\frac{\lambda_n}{\lambda_1}\right)^k {\bf u}_n\right).\]

<p>In the case where one eigenvalue has magnitude that is strictly greater than all the others, i.e.</p>

<p>\(\vert\lambda_1\vert &gt; \vert\lambda_2\vert\geq \vert\lambda_3\vert \geq \dots \geq\vert\lambda_n\vert\),</p>

<p>this implies</p>

\[\lim_{k\to\infty}\frac{\mathbf{A}^k {\bf x}_0}{\lambda_1^{k}} = \alpha_1 {\bf u}_1.\]

<p>This observation motivates the algorithm known as <strong><em>power iteration</em></strong>, which is the topic of the next section.</p>

<h2 id="power-iteration-algorithm">Power Iteration algorithm</h2>

<p>For a matrix \({\bf A}\), power iteration will find a scalar multiple of an eigenvector \({\bf u}_1\), corresponding to the dominant eigenvalue (largest in magnitude) \(\lambda_1\), provided that \(\vert\lambda_1\vert\) is strictly greater than the magnitude of the other eigenvalues, i.e., \(\vert\lambda_1\vert &gt; \vert\lambda_2\vert \ge \dots \ge \vert\lambda_n\vert\).</p>

<p>Suppose</p>

\[\mathbf{x}_0 = \alpha_1 \mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \dots \alpha_n\mathbf{u}_n,\text{ with }\alpha_1 \neq 0.\]

<p>From the previous section, the iterative sequence</p>

\[\mathbf{x}_k = \mathbf{A}\mathbf{x}_{k-1}\text{ for }k=1,2,3,\dots\]

<p>satisfies</p>

\[\mathbf{x}_k = \mathbf{A}^k\mathbf{x}_0 \implies \lim_{k\to\infty}\frac{\mathbf{x}_k}{\lambda_1^k} = \alpha_1\mathbf{u}_1.\]

<p>Thus, for large <span>\(k\)</span>, we have \(\mathbf{x}_k\approx \lambda_1^k \alpha_1\mathbf{u}_1\). Unfortunately, this means that
\(\|\mathbf{x}_k\| \approx |\lambda_1|^k\cdot\|\alpha_1\mathbf{u}_1\|,\)
which will be very large if \(|\lambda_1| &gt; 1\), or very small if <span class="math inline">\(|\lambda_1| &lt; 1\)</span>. For this reason, we use <strong><em>normalized</em></strong> power iteration.</p>

<p>Normalized power iteration, is given by the following. Let \(\mathbf{x}_0\) be a vector with unit norm: \(\|\mathbf{x}_0\| = 1\) (any norm is fine), with \(\mathbf{x}_0 = \alpha_1 \mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \dots \alpha_n\mathbf{u}_n,\text{ and }\alpha_1 \neq 0\).</p>

<p><strong><em>Normalized power iteration</em></strong> is defined by the following iterative sequence for \(k=1,2,3,\dots\):</p>

\[\begin{align} &amp;\mathbf{y}_k = \mathbf{A}\mathbf{x}_{k-1} \\ &amp;\mathbf{x}_k = \frac{\mathbf{y}_k}{\|\mathbf{y}_k\|} \end{align}\]

<p>where the norm \(\|\cdot\|\) is identical to the norm used when we assumed \(\|\mathbf{x}_0\| = 1\).</p>

<p>It can be shown that this sequence satisfies</p>

\[\mathbf{x}_k = \frac{\mathbf{A}^k\mathbf{x}_0}{\|\mathbf{A}^k\mathbf{x}_0\|}.\]

<p>This means that for large values of <span>\(k\)</span>, we have</p>

\[\mathbf{x}_k \approx \left(\frac{\lambda_1}{|\lambda_1|}\right)^k\cdot\frac{\alpha_1\mathbf{u}_1}{\|\alpha_1\mathbf{u}_1\|}.\]

<p>The largest eigenvalue could be positive, negative, or a complex number. In each case we will have:</p>

\[\begin{align} \lambda_1 &gt; 0 \implies &amp;\mathbf{x}_k \approx \frac{\alpha_1\mathbf{u}_1}{\|\alpha_1\mathbf{u}_1\|}\hspace{22.5mm} \mathbf{x}_k\text{ converges} \\ \lambda_1 &lt; 0 \implies &amp;\mathbf{x}_k \approx (-1)^k \frac{\alpha_1\mathbf{u}_1}{\|\alpha_1\mathbf{u}_1\|}\hspace{11.5mm} \text{in the limit, }\mathbf{x}_k\text{ alternates between }\pm\frac{\alpha_1\mathbf{u}_1}{\|\alpha_1\mathbf{u}_1\|}\\ \lambda_1 = re^{i\theta} \implies &amp; \mathbf{x}_k \approx e^{ik\theta} \frac{\alpha_1\mathbf{u}_1}{\|\alpha_1\mathbf{u}_1\|} \hspace{16mm} \text{in the limit, }\mathbf{x}_k \text{ is a scalar multiple of } \mathbf{u}_1 \text{ with coefficient that rotates around the unit circle}.
\end{align}\]

<p>Strictly speaking, normalized power iteration only converges to a single vector if \(\lambda_1 &gt; 0\), but \(\mathbf{x}_k\) will be close to a scalar multiple of the eigenvector \(\mathbf{u}_1\) for large values of <span>\(k\)</span>, regardless of whether the dominant eigenvalue is positive, negative, or complex. So normalized power iteration will work for any value of \(\lambda_1\), as long as it is strictly bigger in magnitude than the other eigenvalues.</p>

<h2 id="power-iteration-code">Power Iteration code</h2>

<p>The following code snippet performs power iteration:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">def</span> <span class="nf">power_iter</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
  <span class="c1"># A: nxn matrix, x_0: initial guess, p: type of norm
</span>  <span class="n">x_0</span> <span class="o">=</span> <span class="n">x_0</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
  <span class="n">x_k</span> <span class="o">=</span> <span class="n">x_0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
    <span class="n">y_k</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">x_k</span>
    <span class="n">x_k</span> <span class="o">=</span> <span class="n">y_k</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y_k</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x_k</span>
</code></pre></div></div>

<h4 id="example-two-steps-of-power-iteration">Example: Two Steps of Power Iteration</h4>

<p>We’ll use normalized power iteration (with the infinity norm) to approximate an eigenvector of the following matrix:
\(\mathbf{A} = \begin{bmatrix} 1 &amp; -2 \\ -1 &amp; 1 \end{bmatrix},\)
and the following initial guess:
\(\mathbf{x}_0 = \begin{bmatrix} -1 \\ 0 \end{bmatrix}\)</p>

<p><strong>First Iteration</strong>:</p>

\[\begin{align} &amp;\mathbf{y}_1 = \mathbf{A}\mathbf{x}_0 = \begin{bmatrix} 1 &amp; -2 \\ -1 &amp; 1 \end{bmatrix} \begin{bmatrix} -1 \\ 0 \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix},\\[15pt] &amp;\mathbf{x}_1 = \frac{\mathbf{y}_1}{\|\mathbf{y}_1\|_{\infty}} = \mathbf{y}_1 = \begin{bmatrix} -1 \\ 1\end{bmatrix}. \end{align}\]

<p><strong>Second Iteration</strong>:</p>

\[\begin{align} &amp;\mathbf{y}_2 = \mathbf{A}\mathbf{x}_1 = \begin{bmatrix} 1 &amp; -2 \\ -1 &amp; 1 \end{bmatrix} \begin{bmatrix} -1 \\ 1 \end{bmatrix} = \begin{bmatrix} -3 \\ 2 \end{bmatrix},\\[15pt] &amp;\mathbf{x}_2 = \frac{\mathbf{y}_2}{\|\mathbf{y}_2\|_{\infty}} = \frac{1}{3}\mathbf{y}_2= \begin{bmatrix} -1 \\ \frac{2}{3}\end{bmatrix} = \begin{bmatrix} -1 \\ 0.6666\dots\end{bmatrix}. \end{align}\]

<p>Even after only two iterations, we are getting close to a corresponding eigenvector:</p>

\[\mathbf{u}_1 = \begin{bmatrix} -1 \\ \frac{1}{\sqrt{2}} \end{bmatrix} \approx \begin{bmatrix} -1 \\ 0.7071 \end{bmatrix}\]

<p>with relative error about 4 percent when measured in the infinity norm.</p>

<h2 id="computing-eigenvalues-from-eigenvectors">Computing Eigenvalues from Eigenvectors</h2>

<p>Power iteration allows us to find an approximate eigenvector corresponding to the largest eigenvalue in magnitude. How can we compute the actual eigenvalue from this?</p>

<p>If \(\lambda\) is an eigenvalue of \(\mathbf{A}\), with corresponding eigenvector \(\mathbf{u}\), then we can compute the value of \(\lambda\) using the <strong><em>Rayleigh Quotient</em></strong>:</p>

\[\lambda = \frac{\mathbf{u}^T\mathbf{A}\mathbf{u}}{\mathbf{u}^T\mathbf{u}}.\]

<p>Thus, one can compute an approximate eigenvalue using the approximate eigenvector found during power iteration.</p>

<h2 id="power-iteration-and-floating-point-arithmetic">Power Iteration and Floating-Point Arithmetic</h2>

<p>Recall that we made the assumption that the initial guess satisfies</p>

\[\mathbf{x}_0 = \alpha_1 \mathbf{u}_1 + \alpha_2\mathbf{u}_2 + \dots \alpha_n\mathbf{u}_n,\text{ with }\alpha_1 \neq 0.\]

<p>What happens if we choose an initial guess where \(\alpha_1 = 0\)? If we further assume that \(\vert\lambda_2\vert &gt; \vert\lambda_3\vert\geq \vert\lambda_4\vert \geq \dots \geq\vert\lambda_n\vert\), then in theory</p>

\[\mathbf{A}^k\boldsymbol{x}_0= \lambda_2^k \left(\alpha_2 {\bf u}_2 + \alpha_3\left(\frac{\lambda_3}{\lambda_2}\right)^k{\bf u}_3 + \dots + \alpha_n \left(\frac{\lambda_n}{\lambda_2}\right)^k {\bf u}_n\right),\]

<p>and we would expect that</p>

\[\lim_{k\to\infty}\frac{\mathbf{A}^k \boldsymbol{x}_0}{\lambda_2^{k}} = \alpha_2 {\bf u}_2.\]

<p>In practice, this does not happen. For one thing, choosing an initial guess such that \(\alpha_1 = 0\) is extremely unlikely if we have no prior knowledge about the eigenvector \(\mathbf{u}_1\). Since power iteration is performed numerically, using finite precision arithmetic, we will encounter the presence of rounding error in every iteration. This means that at every iteration \(k\text{, including }k = 0\), we will instead have</p>

\[\mathbf{A}^k\hat{\boldsymbol{x}}_0= \lambda_1^k \left(\hat{\alpha}_1 \boldsymbol{u}_1 + \hat{\alpha}_2\left(\frac{\lambda_2}{\lambda_1}\right)^k\boldsymbol{u}_2 + \dots + \hat{\alpha}_n \left(\frac{\lambda_n}{\lambda_1}\right)^k \boldsymbol{u}_n\right),\]

<p>where the \(\hat{\alpha}_k\) are the approximate expansion coefficients of the rounded result. Even if \(\alpha_1 = 0\), the finite precision representation \(\hat{\mathbf{x}}_0\), will very likely have expansion coefficient \(\hat{\alpha}_1 \neq 0\). Even in the case where rounding the initial guess does not introduce a non-zero \(\hat{\alpha}_1\), rounding after applying the matrix \(\mathbf{A}\) will almost certainly introduce a non-zero component in the dominant eigenvector after enough iterations. The probability of coming up with a starting guess \(\mathbf{x}_0\) such that \(\hat{\alpha}_1 = 0\) for all iterations is very, very low, if not impossible.</p>

<h2 id="power-iteration-without-a-dominant-eigenvalue">Power Iteration without a Dominant Eigenvalue</h2>

<p>Above, we assumed that one eigenvalue had magnitude strictly larger than all the others: \(\vert\lambda_1\vert &gt; \vert\lambda_2\vert\geq \vert\lambda_3\vert \geq \dots \geq\vert\lambda_n\vert\). What happens if \(\vert\lambda_1\vert = \vert\lambda_2\vert\)?</p>

<p>If \(\lambda_1 = \lambda_2 = \lambda \in \mathbb{R}\), then:</p>

\[\mathbf{x}_k = \mathbf{A}^k\mathbf{x}_0 \approx \alpha_1\lambda^k\mathbf{u}_1 + \alpha_2\lambda^k\mathbf{u}_2 = \lambda^k\left(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2\right),\]

<p>hence</p>

\[\lim_{k\to\infty}\lambda^{-k}\mathbf{A}^k\mathbf{x}_0 = \alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2.\]

<p>The quantity \(\alpha_1\mathbf{u}_1 + \alpha_2\mathbf{u}_2\) is still an eigenvector corresponding to \(\lambda\), so power iteration will still approach a dominant eigenvector.</p>

<p>If the dominant eigenvalues have opposite sign, i.e., \(\lambda_1 = -\lambda_2 = \lambda \in \mathbb{R}\), then</p>

\[\mathbf{x}_k = \mathbf{A}^k\mathbf{x}_0 \approx \alpha_1\lambda^k\mathbf{u}_1 + \alpha_2(-\lambda)^k\mathbf{u}_2 = \lambda^k\left(\alpha_1\mathbf{u}_1 + (-1)^k\alpha_2\mathbf{u}_2\right).\]

<p>For large <span>\(k\)</span>, we will have \(\lambda^{-k}\mathbf{A}\mathbf{x}_0 \approx \alpha_1\mathbf{u}_1 + (-1)^k\alpha_2\mathbf{u}_2\), which although is a linear combination of two eigenvectors, is <strong><em>not</em></strong> itself an eigenvector of \(\mathbf{A}\).</p>

<p>Finally, if the two dominant eigenvalues are a complex-conjugate pair \(\lambda_1 = re^{i\theta},\ \lambda_2 = re^{-i\theta}\), then
\(\mathbf{x}_k = \mathbf{A}^k\mathbf{x}_0 \approx \alpha_1\lambda^k\mathbf{u}_1 + \alpha_2(\overline{\lambda})^k\mathbf{u}_2 = \lambda^k\left(\alpha_1\mathbf{u}_1 + \left(\frac{\overline{\lambda}}{\lambda}\right)^k\alpha_2\mathbf{u}_2\right) = \lambda^k(\alpha_1\mathbf{u}_1 + \alpha_2 e^{-i2k\theta}\mathbf{u}_2).\)</p>

<p>For large <span>\(k\)</span>, \(\lambda^{-k}\mathbf{A}\mathbf{x}_0\) approximate a linear combination of two eigenvectors, but this linear combination will not itself be an eigenvector.</p>

<h2 id="inverse-iteration">Inverse Iteration</h2>

<p>To obtain an eigenvector corresponding to the <strong><em>smallest</em></strong> eigenvalue \(\lambda_n\) of a non-singular matrix, we can apply power iteration to \(\mathbf{A}^{-1}\). The following recurrence relationship describes inverse iteration algorithm:
\(\boldsymbol{x}_{k+1} = \frac{\mathbf{A}^{-1} \boldsymbol{x}_k}{\|\mathbf{A}^{-1} \boldsymbol{x}_k\|}.\) Do not forget to nomalize each \(\boldsymbol{x}_{k+1}.\)</p>

<h2 id="inverse-iteration-with-shift">Inverse Iteration with Shift</h2>

<p>To obtain an eigenvector corresponding to the eigenvalue closest to some value \(\sigma\), \(\mathbf{A}\) can be shifted by \(\sigma\) and inverted in order to solve it similarly to the power iteration algorithm. The following recurrence relationship describes inverse iteration algorithm:
\(\boldsymbol{x}_{k+1} = \frac{(\mathbf{A} - \sigma \mathbf{I})^{-1} \boldsymbol{x}_k}{\|(\mathbf{A} - \sigma \mathbf{I})^{-1} \boldsymbol{x}_k\|}\). Note that this is identical to inverse iteration if the shift is zero.</p>

<h2 id="rayleigh-quotient-iteration">Rayleigh Quotient Iteration</h2>

<p>The shift \(\sigma\) can be updated based on a current estimate of the eigenvalue in order to improve convergence rate. Such an estimate can be found using the Rayleigh Quotient. Rayleigh Quotient Iteration is given by the following recurrence relation:</p>

\[\sigma_k = \frac{\boldsymbol{x}_k^T \mathbf{A} \boldsymbol{x}_k}{\boldsymbol{x}_k^T\boldsymbol{x}_k}\]

\[\boldsymbol{x}_{k+1} = \frac{(\mathbf{A} - \sigma_k \mathbf{I})^{-1} \boldsymbol{x}_k}{\|(\mathbf{A} - \sigma_k \mathbf{I})^{-1} \boldsymbol{x}_k\|}.\]

<h2 id="convergence-properties">Convergence properties</h2>

<p>The convergence rate for power iteration is <strong><em>linear</em></strong> and the recurrence relationship for the error between the current iterate and a dominant eigenvector is given by:
\(\mathbf{e}_{k+1} \approx \frac{|\lambda_2|}{|\lambda_1|}\mathbf{e}_k\).</p>

<p>The convergence rate for (shifted) inverse iteration is also linear, but now depends on the two closest eigenvalues to the shift \(\sigma\). (Remember that standard inverse iteration corresponds to a shift \(\sigma = 0\).) The recurrence relationship for the errors is given by:
\(\mathbf{e}_{k+1} \approx \frac{|\lambda_\text{closest} - \sigma|}{|\lambda_\text{second-closest} - \sigma|}\mathbf{e}_k.\)</p>

<h2 id="cost-summary">Cost Summary</h2>

<p>(a) Power Method \(\boldsymbol{x}_{k+1} = \mathbf{A} \boldsymbol{x}_{k}\), the cost is \(kn^2\). <br />
(b) Inverse Power Method \(\mathbf{A} \boldsymbol{x}_{k+1} = \boldsymbol{x}_{k}\), the cost is \(n^{3} + kn^2\). <br />
(c) Shifted Inverse Power Method \((\mathbf{A} - \sigma \mathbf{I}) \boldsymbol{x}_{k+1} = \boldsymbol{x}_{k}\), the cost is \(n^{3} + kn^2\).</p>

<h2 id="orthogonal-matrices">Orthogonal Matrices</h2>

<p>Square matrices are called <strong><em>orthogonal</em></strong> if and only if the columns are mutually orthogonal to one another and have a norm of <span>\(1\)</span> (such a set of vectors are formally known as an <strong><em>orthonormal</em></strong> set), i.e.:
\(\boldsymbol{c}_i^T \boldsymbol{c}_j = 0 \quad \forall \ i \neq j, \quad \|\boldsymbol{c}_i\| = 1 \quad \forall \ i \iff \mathbf{A} \in \mathcal{O}(n),\)
or
\(\langle\boldsymbol{c}_i,\boldsymbol{c}_j \rangle = \begin{cases} 0 \quad \mathrm{if} \ i \neq j, \\ 1 \quad \mathrm{if} \ i = j \end{cases} \iff \mathbf{A} \in \mathcal{O}(n),\)
where \(\mathcal{O}(n)\) is the set of all \(n \times n\) orthogonal matrices called the orthogonal group, \(\boldsymbol{c}_i\), \(i=1, \dots, n\), are the columns of <span>\(\mathbf{A}\)</span>, and \(\langle \cdot, \cdot \rangle\) is the inner product operator. Orthogonal matrices have many desirable properties:<br />
(a) \(\mathbf{A}^T \in \mathcal{O}(n)\)<br />
(b) \(\mathbf{A}^T \mathbf{A} = \mathbf{A} \mathbf{A}^T = \mathbf{I} \implies \mathbf{A}^{-1} = \mathbf{A}^T\)<br />
(c) \(\det{\mathbf{A}} = \pm 1\)<br />
(d) \(\kappa_2(\mathbf{A}) = 1\)</p>

<h2 id="gram-schmidt">Gram-Schmidt</h2>

<p>The algorithm to construct an orthogonal basis from a set of linearly independent vectors is called the Gram-Schmidt process. For a basis set \(\{x_1, x_2, \dots x_n\}\), we can form an orthogonal set \(\{v_1, v_2, \dots v_n\}\) given by the following transformation:<br />
\(\begin{align} \boldsymbol{v}_1 &amp;= \boldsymbol{x}_1, \\ \boldsymbol{v}_2 &amp;= \boldsymbol{x}_2 - \frac{\langle\boldsymbol{v}_1,\boldsymbol{x}_2\rangle}{\|\boldsymbol{v}_1\|^2}\boldsymbol{v}_1,\\ \boldsymbol{v}_3 &amp;= \boldsymbol{x}_3 - \frac{\langle\boldsymbol{v}_1,\boldsymbol{x}_3\rangle}{\|\boldsymbol{v}_1\|^2}\boldsymbol{v}_1 - \frac{\langle\boldsymbol{v}_2,\boldsymbol{x}_3\rangle}{\|\boldsymbol{v}_2\|^2}\boldsymbol{v}_2,\\ \vdots &amp;= \vdots\\ \boldsymbol{v}_n &amp;= \boldsymbol{x}_n - \sum_{i=1}^{n-1}\frac{\langle\boldsymbol{v}_i,\boldsymbol{x}_n\rangle}{\|\boldsymbol{v}_i\|^2}\boldsymbol{v}_i, \end{align}\)<br />
where \(\langle \cdot, \cdot \rangle\) is the inner product operator. Each of the vectors in the new orthogonal set \(\{v_1, v_2, \dots v_n\}\) can be <strong>normalized</strong> independently to obtain an <strong>orthonormal basis</strong>.</p>

<h2 id="review-questions">Review Questions</h2>

<ul>
  <li>See this <a href="/cs357/fa2020/reviews/rev-12-eigen.html">review link</a></li>
</ul>

<h2 id="changelog">ChangeLog</h2>

<ul>
  <li>2022-02-28 Yuxuan Chen <a href="mailto:yuxuan19@illinois.edu">yuxuan19@illinois.edu</a>: added learning objectives, cost summary</li>
  <li>2020-03-01 Peter Sentz: added text to include content from slides</li>
  <li>2018-10-14 Erin Carrier <a href="mailto:ecarrie2@illinois.edu">ecarrie2@illinois.edu</a>: removes orthogonal/GS sections</li>
  <li>2018-01-14 Erin Carrier <a href="mailto:ecarrie2@illinois.edu">ecarrie2@illinois.edu</a>: removes demo links</li>
  <li>2017-11-10 Erin Carrier <a href="mailto:ecarrie2@illinois.edu">ecarrie2@illinois.edu</a>: adds costs of methods</li>
  <li>2017-10-26 Matthew West <a href="mailto:mwest@illinois.edu">mwest@illinois.edu</a>: rewrote eval/evec definitions</li>
  <li>2017-10-25 Erin Carrier <a href="mailto:ecarrie2@illinois.edu">ecarrie2@illinois.edu</a>: minor fixes, added review questions</li>
  <li>2017-10-14 Arun Lakshmanan <a href="mailto:lakshma2@illinois.edu">lakshma2@illinois.edu</a>: first complete draft</li>
  <li>2017-10-16 Luke Olson <a href="mailto:lukeo@illinois.edu">lukeo@illinois.edu</a>: outline</li>
</ul>

    </div>
</section>

</body>
<script src="//assets/js/jquery-1.12.1.min.js"></script>
<!-- popper js -->
<script src="//assets/js/popper.min.js"></script>
<!-- bootstrap js -->
<script src="//assets/js/bootstrap.min.js"></script>
<!-- easing js -->
<script src="//assets/js/jquery.magnific-popup.js"></script>
<!-- swiper js -->
<script src="//assets/js/swiper.min.js"></script>
<!-- swiper js -->
<script src="//assets/js/masonry.pkgd.js"></script>
<!-- particles js -->
<script src="//assets/js/owl.carousel.min.js"></script>
<script src="//assets/js/jquery.nice-select.min.js"></script>
<!-- swiper js -->
<script src="//assets/js/slick.min.js"></script>
<script src="//assets/js/jquery.counterup.min.js"></script>
<script src="//assets/js/waypoints.min.js"></script>
<!-- custom js -->
<script src="//assets/js/custom.js"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</html>
